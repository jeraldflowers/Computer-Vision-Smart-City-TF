{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIPg3hldUBI+oIWe1bxK0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeraldflowers/Computer-Vision-Smart-City-TF/blob/main/json_tfrecord.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLhngUCkG7IM",
        "outputId": "d07b45ce-f163-42a5-d16e-b8e4f925dfc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 133 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 184 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 215 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 235 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 266 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 317 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 348 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 352 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (1.2.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pickle\n",
        "import zipfile\n",
        "\n",
        "!pip install tf_slim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eC3l2CigMKyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_zip = \"/content/dataset_original_.zip\"\n",
        "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "zip_ref.extractall(\"dataset_original_\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "KEy031ePJTKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_file = \"train\"\n",
        "path = \"/content/train_ready.json\"\n",
        "data_file = open(path)\n",
        "data = json.load(data_file)\n"
      ],
      "metadata": {
        "id": "E2yL_dJ5Js-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type_file = \"test\"\n",
        "path = \"/content/test_ready.json\"\n",
        "data_file = open(path)\n",
        "data = json.load(data_file) "
      ],
      "metadata": {
        "id": "qGkhX25uPYVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_list = []\n",
        "\n",
        "for classification in data:\n",
        "  width, height = classification['width'], classification['height']\n",
        "  image = classification['image']\n",
        "  for item in classification['tags']:\n",
        "    name = item['name']\n",
        "    xmin = item['pos']['x']\n",
        "    ymin = item['pos']['y']\n",
        "    xmax = xmin + item['pos']['w']\n",
        "    ymax = ymin + item['pos']['h']\n",
        "\n",
        "    value = (image, width, height, name, xmin, ymin, xmax, ymax)\n",
        "    csv_list.append(value)\n",
        "\n",
        "column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "csv_df = pd.DataFrame(csv_list, columns = column_name)\n",
        "\n",
        "csv_df.to_csv(\"/content/{}_labels.csv\".format(type_file))"
      ],
      "metadata": {
        "id": "Dp5gIR8FKYY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install TFrecord"
      ],
      "metadata": {
        "id": "mmUn42T0P6Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "%cd /content/models/\n",
        "!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n",
        "!apt-get update && apt-get install -y -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "metadata": {
        "id": "96H4-PKYPxl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.record"
      ],
      "metadata": {
        "id": "8IObD4-AUK_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.append(\"../../models/research\")\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "# for multiple labels add more else if statements\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == \"carro\":  # 'ship':\n",
        "        return 1\n",
        "    elif row_label == \"motos\":\n",
        "        return 2    \n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    # check if the image format is matching with your images.\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "output_path = \"train.record\"\n",
        "image_dir = \"/content/dataset_original/dataset_original\"\n",
        "csv_input = \"/content/train_labels.csv\"\n",
        "\n",
        "writer = tf.io.TFRecordWriter(output_path)\n",
        "path = os.path.join(image_dir)\n",
        "examples = pd.read_csv(csv_input)\n",
        "grouped = split(examples, 'filename')\n",
        "for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "\n",
        "writer.close()\n",
        "output_path = os.path.join(os.getcwd(), output_path)\n",
        "print('Successfully created the TFRecords: {}'.format(output_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bGvVT9vQQFn",
        "outputId": "f4d2da6e-fe43-496c-b7b8-2efcf10c4267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecords: /content/models/research/train.record\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## test.record"
      ],
      "metadata": {
        "id": "jXLIIYVVUTeR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S2fPwonyT1FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.append(\"../../models/research\")\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "# for multiple labels add more else if statements\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == \"carro\":  # 'ship':\n",
        "        return 1\n",
        "    elif row_label == \"motos\":\n",
        "        return 2    \n",
        "    else:\n",
        "        None\n",
        "\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    # check if the image format is matching with your images.\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "output_path = \"test.record\"\n",
        "image_dir = \"/content/dataset_original/dataset_original\"\n",
        "csv_input = \"/content/test_labels.csv\"\n",
        "\n",
        "writer = tf.io.TFRecordWriter(output_path)\n",
        "path = os.path.join(image_dir)\n",
        "examples = pd.read_csv(csv_input)\n",
        "grouped = split(examples, 'filename')\n",
        "for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "\n",
        "writer.close()\n",
        "output_path = os.path.join(os.getcwd(), output_path)\n",
        "print('Successfully created the TFRecords: {}'.format(output_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1862eb9-f4b3-401e-88e9-c82d25eb7ce5",
        "id": "U3072BhKUWGp"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully created the TFRecords: /content/models/research/test.record\n"
          ]
        }
      ]
    }
  ]
}